# -*- coding: utf-8 -*-
"""
Automatically generated by Colab.

---
### Mata Kuliah Praktikum Deep Learning

### **Proyek UTS DEEP LEARNING**


---

#**Tahapan Deep Learning**


---
## **Judul : "Klasifikasi Genre Musik Berbasis Visual Spectrogram Menggunakan *Convolutional Neural Network* (CNN) dengan Pendekatan Transfer Learning"**


---


### ***Acquiring Data/Data Collecting* ((Unduh dan Persiapkan Data)**
"""

# Tensorflow Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import image_dataset_from_directory

# Plotting and Model Evaulation Libraries
import matplotlib.pyplot as plt
import IPython.display as ipd
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay

# Utility Libraries
import numpy as np
import os

# Setting seed for reproducibility
keras.utils.set_random_seed(42)

"""#### Load Data

Sekarang kita memuat spektrogram dari folder kita, dan membaginya menjadi set pelatihan, validasi, dan pengujian.

####  Mount Google Drive & Setup
"""

from google.colab import drive
drive.mount('/content/drive')

"""### ***Preprocessing***"""

training_images_filepath = "/content/drive/MyDrive/Music/Data/images_original"
genre_counts = {genre: len(os.listdir(os.path.join(training_images_filepath, genre))) for genre in os.listdir(training_images_filepath)}
print(genre_counts)
category_labels = os.listdir(training_images_filepath)

xdim = 180
ydim = 180

spectograms = image_dataset_from_directory(
    training_images_filepath,
    image_size = (xdim, ydim),
    batch_size = 111)

## Use num_batches - 2 batches for training, 1 batch for validation, 1 batch for testing
num_batches = tf.data.experimental.cardinality(spectograms).numpy()
train = spectograms.take(num_batches - 2).cache()
remaining = spectograms.skip(num_batches - 2)
validation = remaining.take(1).cache()
test = remaining.skip(1).cache()

"""Sebelum melanjutkan, kita dapat melihat seperti apa tampilan beberapa spektrogram kita. Di bawah ini kita akan menampilkan lima spektrogram pertama dalam set validasi dan labelnya yang sesuai."""

for images, labels in validation:
    plt.figure(figsize=(15, 500))
    for i in range(5):
        plt.subplot(1, 5, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(f"Label: {category_labels[labels[i].numpy()]}")
        plt.axis("off")
    plt.show()

# Preprocessing: Data Augmentation
from tensorflow.keras.preprocessing import image

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip('horizontal'),
    tf.keras.layers.RandomRotation(0.2),
    tf.keras.layers.RandomZoom(0.2),
])

# Preprocess input images (resize and augment)
def preprocess_data(images):
    images = data_augmentation(images)  # Augmentasi
    return keras.applications.vgg16.preprocess_input(images)

"""### ***Splitting and Balancing the Dataset***

Bagi dataset menjadi train, validation, dan test. Cek keseimbangan kelas jika perlu.
"""

import collections

# Fungsi untuk menghitung distribusi label
def count_labels(dataset):
    labels = []
    for _, label in dataset.unbatch():
        labels.append(label.numpy())
    return dict(collections.Counter(labels))

# Hitung distribusi kelas
train_dist = count_labels(train)
val_dist = count_labels(validation)
test_dist = count_labels(test)

# Tampilkan hasil dengan label kategori
print("Distribusi Train:")
for i in range(len(category_labels)):
    print(f"{category_labels[i]}: {train_dist.get(i, 0)}")

print("\nDistribusi Validation:")
for i in range(len(category_labels)):
    print(f"{category_labels[i]}: {val_dist.get(i, 0)}")

print("\nDistribusi Test:")
for i in range(len(category_labels)):
    print(f"{category_labels[i]}: {test_dist.get(i, 0)}")

"""### **Desain Arsitektur CNN-->*Building and Training the Model***

#### Model Transfer Learning

Sekarang kita akan mulai membangun model untuk mengklasifikasikan spektrogram. Untuk melakukannya, kita akan memuat jaringan saraf VGG16 dan menyempurnakannya agar sesuai dengan kebutuhan kita.
"""

conv_base = keras.applications.vgg16.VGG16(
    weights = "imagenet",
    include_top = False,
    input_shape = (xdim, ydim, 3))
conv_base.summary()

"""Untuk langkah pertama penyempurnaan, kita akan membekukan model dasar dan menambahkan head kita sendiri untuk dilatih. Artinya, kita akan menambahkan lapisan kita sendiri ke model VGG16 dan HANYA melatih bobot lapisan baru sambil mempertahankan bobot lapisan asli di VGG16 tetap sama. Untuk saat ini, kita hanya akan menambahkan head sederhana dengan 256 node dan fungsi aktivasi relu.

Perhatikan bahwa tanpa GPU, setiap epoch membutuhkan waktu sekitar 110 detik untuk berjalan.

##### Feature Extraction (Freeze base model)
"""

# Freeze the layers
conv_base.trainable = False

# Define the new head
inputs = keras.Input(shape=(xdim, ydim, 3))
x = keras.applications.vgg16.preprocess_input(inputs)
x = conv_base(inputs)
x = layers.Flatten()(x)
x = layers.Dense(256, activation = "relu")(x)
outputs = layers.Dense(len(category_labels), activation="softmax")(x)
model = keras.Model(inputs, outputs)

model.compile(loss="sparse_categorical_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])

history = model.fit(train,
    epochs = 25,
    validation_data = validation,
    verbose = 1)

# Plotting loss and accuracy graphs
# Loss
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss (25 Epochs)')
plt.plot(history.history['val_loss'], label='Val Loss (25 Epochs)')
plt.title('Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(["Training Set", "Validation Set"], loc='upper left')
plt.tight_layout()
plt.show()

# Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy (25 Epochs)')
plt.plot(history.history['val_accuracy'], label='Val Accuracy (25 Epochs)')
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(["Training Set", "Validation Set"], loc='upper left')

plt.tight_layout()
plt.show()

"""Kini setelah kepala model kita cukup terlatih, kita selanjutnya menyempurnakan model dengan mencairkan empat lapisan terakhir model VGG16 dan menyempurnakannya ditambah lapisan kepala.

##### Fine-tuning (Unfreeze last layers)
"""

conv_base.trainable = True
for layer in conv_base.layers[:-4]:
    layer.trainable = False

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),
              metrics=["accuracy"])

history = model.fit(
    train,
    epochs = 25,
    validation_data = validation,
    verbose = 1)

# Plotting loss and accuracy graphs
# Loss
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(["Training Set", "Validation Set"], loc='upper left')
plt.tight_layout()
plt.show()

# Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(["Training Set", "Validation Set"], loc='upper left')

plt.tight_layout()
plt.show()

"""
### **Evaluasi Model**

Sekarang setelah kita memiliki model, kita dapat menguji kinerjanya pada set pengujian."""

predictions_prob = model.predict(test)
predictions = np.argmax(predictions_prob, axis = 1)

ground_truth = [label for _, label in test.unbatch()]
ground_truth = tf.stack(ground_truth, axis = 0).numpy()

accuracy = accuracy_score(ground_truth, predictions)
print("Accuracy of the model:", accuracy)

"""Kami memperoleh akurasi sekitar 90%, yang sangat kuat. Kami dapat menganalisis kinerja lebih lanjut dengan mengevaluasi matriks kebingungan."""

fig, ax = plt.subplots(figsize=(12,8))
conf_matrix = confusion_matrix(ground_truth, predictions)
ConfusionMatrixDisplay(conf_matrix, display_labels = category_labels).plot(ax = ax)

report = classification_report(ground_truth, predictions)
print('\nClassification Report:\n', report)

# Kalkulasi precision, recall, and F1-score
from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(ground_truth, predictions, average='weighted')
recall = recall_score(ground_truth, predictions, average='weighted')
f1 = f1_score(ground_truth, predictions, average='weighted')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
#Visualisasi Confusion Matrix
ConfusionMatrixDisplay(conf_matrix, display_labels = category_labels).plot(ax = ax)
plt.show()

"""### ***Hyperparameter***"""

!pip install keras-tuner

import keras_tuner as kt

def build_model_tune(hp):
    conv_base = keras.applications.vgg16.VGG16(
        weights="imagenet", include_top=False, input_shape=(xdim, ydim, 3))
    conv_base.trainable = False

    inputs = keras.Input(shape=(xdim, ydim, 3))
    x = keras.applications.vgg16.preprocess_input(inputs)
    x = conv_base(x)
    x = layers.Flatten()(x)

    # Hyperparameter tuning untuk layer Dense
    x = layers.Dense(
        units=hp.Int('units', min_value=128, max_value=1024, step=128),
        activation='relu'
    )(x)

    outputs = layers.Dense(len(category_labels), activation="softmax")(x)
    model = keras.Model(inputs, outputs)

    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=keras.optimizers.Adam(
            learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')
        ),
        metrics=["accuracy"]
    )

    return model

# Hyperparameter tuning using Random Search
tuner = kt.RandomSearch(
    build_model_tune,
    objective="val_accuracy",
    max_trials=5,
    executions_per_trial=1,
    directory='project_dir',
    project_name='music_genre_tuning'
)

tuner.search(train, epochs=25, validation_data=validation)

"""### **Menyimpan Model**"""

# Menyimpan model
model.save('music_genre.h5')

"""### **Coba Testing/Uji untuk Inputan di Website**"""

from google.colab import files

uploaded = files.upload()

from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input
import numpy as np
import matplotlib.pyplot as plt

def predict_uploaded_image(filename, model, classes):
    img = image.load_img(filename, target_size=(180, 180))  # ukuran sesuai model
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)

    # Preprocessing sesuai dengan VGG16
    img_array = preprocess_input(img_array)

    predictions = model.predict(img_array)
    predicted_index = np.argmax(predictions)
    predicted_label = classes[predicted_index]
    confidence = np.max(predictions) * 100

    print(f"Predicted Genre: {predicted_label} ({confidence:.2f}%)")
    plt.imshow(img_array[0].astype('uint8'))
    plt.title(f'Predicted: {predicted_label} ({confidence:.2f}%)')
    plt.axis('off')
    plt.show()

    return dict(zip(classes, predictions[0]))  # menampilkan semua skor

# Contoh penggunaan (ubah 'nama_file.png' sesuai upload)
filename = list(uploaded.keys())[0]
predictions = predict_uploaded_image(filename, model, category_labels)
print(predictions)